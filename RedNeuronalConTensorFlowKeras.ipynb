{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "RedNeuronalConTensorFlowKeras.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bguzman012/MisProyectos-D/blob/master/RedNeuronalConTensorFlowKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6hWtVm2k-ig",
        "colab_type": "text"
      },
      "source": [
        "# ![LOGOUPS.png](attachment:LOGOUPS.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMs84DCMI82d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQiQk8Ook-ii",
        "colab_type": "text"
      },
      "source": [
        "# TAREA REDES NEURONALES\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNbGM1csk-il",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">1. CARGA DE DATASET</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaxHpRSqk-io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow==2.0.0-beta0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuDBGz_Ik-i7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "02b65a30-f2ca-4048-a3be-2dd14cc42022"
      },
      "source": [
        "#Esta guía explica el proceso para crear una arquitectura de redes neuronales, posteriormente explica el proceso de \n",
        "#optimización y fine tuning. Para ello, primeramente se define una red neuronal y se evalúa. Posteriormente, se ajustan\n",
        "#los parámetros y se evalúa el modelo final.\n",
        "\n",
        "#El dataset se llama Titanic, lo pueden encontrar en: \n",
        "#Dataset Kaggle: https://www.kaggle.com/c/titanic\n",
        "#También lo adjunto en el directorio de esta guía\n",
        "\n",
        "#Con este problema se intenta predecir si una persona sobrevivió o no sobrevivió, este tipo de problemas es aplicable a otros \n",
        "#problemas de sobrevivencia ante desastres.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "etiquetas = [\"buying\",\"maint\",\"doors\",\"persons\",\"lug_boot\",\"safety\",\"y\"]\n",
        "\n",
        "df = pd.read_csv('/carsito.csv', names=etiquetas, header=None)\n",
        "df.head()\n",
        "\n",
        "#Aquí solo con la vista podemos descartar muchas variables que son las siguientes:\n",
        "#Nombre: No nos sirve porque no aporta información al resultado\n",
        "#Ticket: Tampoco nos aporta Información\n",
        "#Cabin: Podría aportarnos información pero tiene valores NaN (perdidos)\n",
        "#PassangerID: Es el indice, no aporta información relevante\n",
        "#Parch: Lo eliminaremos para tener una peor rendimiento para saber como optimizar el modelo en la siguiente sección\n",
        "\n",
        "#Entonces nuestros datos de entrada “X” tendrán los siguientes valores:\n",
        "#Sex: Genero de la persona\n",
        "#Age: Edad de la persona\n",
        "#SibSp: la cantidad de hermanos o esposa\n",
        "#Fare: El impuesto que pagaron para embarcarse\n",
        "#Embarked: Es la clase en donde se embarco la persona"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>buying</th>\n",
              "      <th>maint</th>\n",
              "      <th>doors</th>\n",
              "      <th>persons</th>\n",
              "      <th>lug_boot</th>\n",
              "      <th>safety</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   buying  maint  doors  persons  lug_boot  safety  y\n",
              "0       4      4      2        2         1       1  0\n",
              "1       4      4      2        2         1       2  0\n",
              "2       4      4      2        2         1       3  0\n",
              "3       4      4      2        2         2       1  0\n",
              "4       4      4      2        2         2       2  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grlRoewDk-jJ",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">2. PREPARACION DE DATOS - LIMPIEZA</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2_B_o92k-jN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "eef153a7-0447-474b-b333-288c2c276c25"
      },
      "source": [
        "df = df.dropna(subset=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'y'])\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>buying</th>\n",
              "      <th>maint</th>\n",
              "      <th>doors</th>\n",
              "      <th>persons</th>\n",
              "      <th>lug_boot</th>\n",
              "      <th>safety</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   buying  maint  doors  persons  lug_boot  safety  y\n",
              "0       4      4      2        2         1       1  0\n",
              "1       4      4      2        2         1       2  0\n",
              "2       4      4      2        2         1       3  0\n",
              "3       4      4      2        2         2       1  0\n",
              "4       4      4      2        2         2       2  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W7zSshKk-jX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "fe01769b-bf18-4c53-fd11-1fb57fed038d"
      },
      "source": [
        "#Seleccionamos las variables escogidas\n",
        "\n",
        "\n",
        "\n",
        "Xsubset = df[['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']]\n",
        "#Xsubset = df[['Pclass', 'Sex', 'Embarked']]\n",
        "#Xsubset.fillna(0)\n",
        "\n",
        "#para separar nuestra variable dependiente de la independiente, haremos lo siguiente:\n",
        "\n",
        "y = df.y.values\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "encoded_y = encoder.transform(y)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "y = np_utils.to_categorical(encoded_y)\n",
        "print(Xsubset)\n",
        "print(y)\n",
        "type(Xsubset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      buying  maint  doors  persons  lug_boot  safety\n",
            "0          4      4      2        2         1       1\n",
            "1          4      4      2        2         1       2\n",
            "2          4      4      2        2         1       3\n",
            "3          4      4      2        2         2       1\n",
            "4          4      4      2        2         2       2\n",
            "...      ...    ...    ...      ...       ...     ...\n",
            "1723       1      1      5        5         2       2\n",
            "1724       1      1      5        5         2       3\n",
            "1725       1      1      5        5         3       1\n",
            "1726       1      1      5        5         3       2\n",
            "1727       1      1      5        5         3       3\n",
            "\n",
            "[1728 rows x 6 columns]\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNxg7-5tk-jl",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">3. PREPARACION DE DATOS - PREPROCESAMIENTO</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x5d-MTKk-jn",
        "colab_type": "text"
      },
      "source": [
        "Generalmente cuando entrenamos modelos de “Deep Learning” antes de pasar los datos, todos los valores numéricos hay que normalizarlos, porque si tenemos valores muy altos y muy bajos en la misma tabla, al pasar por la función de activación, dependiendo cual sea, no lanzará resultados óptimos, por lo que se necesita un escalado, para que los valores queden a la misma escala, usaremos el StandarScaler, que es para valores, que tienen mucha diferencia entre ambos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJy2jilik-jq",
        "colab_type": "text"
      },
      "source": [
        "Embarked, Sex, y PClass, son valores categóricos nominales, es decir, son valores que pertenecen a categorías, por ejemplo, PClass representa la clase en que estaban embarcados los pasajeros, sin embargo, los valores son nominales puesto que un pasajero de primera clase no representa mas que uno de segunda, a efectos prácticos, a nivel matemático, si lo tratamos como un numero no suma ni resta. Podemos tratarlo como una categoría, porque tenemos una cantidad fija de clases a lo largo de los datos. Lo mismo pasa con Embarked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybzFeQAGk-jx",
        "colab_type": "text"
      },
      "source": [
        "Por ser valores categóricos nominales no podemos dar las categorías directamente, tenemos que realizar el proceso de Coding es decir, pasar las categorías en formato (one_hot), que consiste en poner tantos ceros como categorías, y para representar un valor se coloca un uno en la posición del valor, ejemplo:\n",
        "\n",
        "3 colores: azul, rojo, verde. la primera categoría corresponde a tantos ceros como tantas categorías que se tienen: \n",
        "azul [1,0,0]\n",
        "verde es el tercero, por lo que para representar ese valor irá un uno en esa posición\n",
        "verde = [0,0,1]\n",
        "\n",
        "Documentación ColumnTransformer: \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
        "\n",
        "Creamos un Pipeline (un formateador o canal) de pre procesamiento con sk_learn, usaremos la función make_column_transformer\n",
        "Link para aprender más: https://scikit-learn.org/stable/modules/compose.html\n",
        "\n",
        "Los transformadores generalmente se combinan con clasificadores, regresores u otros estimadores para construir un estimador compuesto. La herramienta más común es un Pipeline (tubería).\n",
        "\n",
        "Otro enlace con ejemplo: https://medium.com/vickdata/easier-machine-learning-with-the-new-column-transformer-from-scikit-learn-c2268ea9564c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E0c-zT5k-kE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "91fb1473-6d26-4160-82b9-12122dc9562a"
      },
      "source": [
        "preprocesador1 = make_column_transformer(\n",
        "    (StandardScaler(),['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']))\n",
        "\n",
        "X = preprocesador1.fit_transform(Xsubset)\n",
        "print(X.shape[1])\n",
        "print(X.shape)\n",
        "\n",
        "#print(X)\n",
        "\n",
        "#print(preprocesador1)\n",
        "\n",
        "cnamesDataset1 = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']\n",
        "\n",
        "print(cnamesDataset1)\n",
        "\n",
        "DatasetPreprocesado = pd.DataFrame(data=X,columns=cnamesDataset1)\n",
        "print(DatasetPreprocesado.head())\n",
        "\n",
        "DatasetPreprocesado.to_csv(\"/\"+\"DatasetPreprocesado.csv\", sep=\";\",index = False) #sep es el separado, por defector es \",\""
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "(1728, 6)\n",
            "['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']\n",
            "     buying     maint     doors   persons  lug_boot    safety\n",
            "0  1.341641  1.341641 -1.341641 -1.336306 -1.224745 -1.224745\n",
            "1  1.341641  1.341641 -1.341641 -1.336306 -1.224745  0.000000\n",
            "2  1.341641  1.341641 -1.341641 -1.336306 -1.224745  1.224745\n",
            "3  1.341641  1.341641 -1.341641 -1.336306  0.000000 -1.224745\n",
            "4  1.341641  1.341641 -1.341641 -1.336306  0.000000  0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s14cz0Hk-kV",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">4. DIVISION EN TRAIN Y TEST</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfWS43Gjk-kX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5bbdfb0-ba13-495d-ae43-d1af61e069e8"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
        "\n",
        "#Ahora preparamos el perceptron. Importamos las neuronas simples y el modelo secuencial\n",
        "#Modelo secuencial quiere decir que agregaremos capas y se conectarán de manera automática, \n",
        "#Dense es la librería de neuronas simples.\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.models import model_from_json\n",
        "print('Librerías importadas')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Librerías importadas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL9FOC4Zk-ki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f178cb6a-d628-437e-e59e-1a1d031230e3"
      },
      "source": [
        "X_train.shape[1]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFpF_QLbk-ks",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">5. ALMACENAMIENTO Y CARGA DE MODELOS DE REDES NEURONALES</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht1-ClRUk-kx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FUNCIONES PARA GuARDAR Y CARGAR CUALQUIER MODELO\n",
        "\n",
        "#Guardar pesos y la arquitectura de la red en un archivo \n",
        "\n",
        "def guardarRNN(model,nombreArchivoModelo,nombreArchivoPesos):\n",
        "    print(\"Guardando Red Neuronal en Archivo\")  \n",
        "    # serializar modelo a JSON\n",
        "\n",
        "    # Guardar los Pesos (weights)\n",
        "    model.save_weights('/'+ nombreArchivoPesos+'.h5')\n",
        "\n",
        "    # Guardar la Arquitectura del modelo\n",
        "    with open('/' + nombreArchivoModelo+'.json', 'w') as f:\n",
        "        f.write(model.to_json())\n",
        "\n",
        "    print(\"Red Neuronal Grabada en Archivo\")   \n",
        "    \n",
        "def cargarRNN(nombreArchivoModelo,nombreArchivoPesos):\n",
        "        \n",
        "    # Cargar la Arquitectura desde el archivo JSON\n",
        "    with open('/' + nombreArchivoModelo+'.json', 'r') as f:\n",
        "        model = model_from_json(f.read())\n",
        "\n",
        "    # Cargar Pesos (weights) en el nuevo modelo\n",
        "    model.load_weights('/' + nombreArchivoPesos+'.h5')  \n",
        "\n",
        "    print(\"Red Neuronal Cargada desde Archivo\") \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw_0MC4rk-k-",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">6. DISEÑO DE RED NEURONAL DE PRUEBA Y EVALUACION</span> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75MCjSjFk-lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "f134dac6-a3ec-49be-a55a-6e8e9cc43a14"
      },
      "source": [
        "#Construcción del Modelo o Arquitectura de Redes Neoronales\n",
        "model = Sequential()\n",
        "\n",
        "#La primera capa Dense recibe el numero de variables, que es la segunda dimensión de la matriz X, esto es X_train.shape[1]\n",
        "#La primera capa tiene 32 neuronas. La función de activación es la función rectificadora.\n",
        "model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "#La segunda capa tiene 64 neuronas. La función de activación es la función rectificadora.\n",
        "model.add(Dense(64, activation='relu'))\n",
        "#La capa de salida tiene 1 neurona. La capa de salida debe tener la misma dimensión como de cantidad de salidas queremos,\n",
        "#por ejemplo, en este caso la salida \"Survived\" solo requiere 0 y 1. Puesto que 0 o 1 ocupan solo un valor dentro de cada dato,\n",
        "#entonces 1 neurona es suficiente. La función de activación es sigmoide para clasificación por probabilidad.\n",
        "model.add(Dense(4, activation='sigmoid'))\n",
        "\n",
        "#Como tenemos dos posibles salidas \"0 o 1\", vamos a escoger que el error lo trate como una clasificación binaria, \n",
        "#el optimizador será nuestra función derivada que nos ayudará a determinar hacia donde mover los pesos.\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['acc']) #ADADELTA: An Adaptive Learning Rate Method\n",
        "\n",
        "#imprimir arquitectura de la red\n",
        "model.summary()\n",
        "#Entrenamiento: \n",
        "\n",
        "#Entrenaremos por 100 epochs, el batch_size es un argumento importante, porque representa cada cuántos datos va a actualizar\n",
        "#los pesos. Este es el método del gradiente descendiente estocástico que hace el proceso más eficiente y preciso.\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=0)\n",
        "score = model.evaluate(X_train, y_train, verbose=0)\n",
        "print('Resultado en Train:')\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
        "\n",
        "#Fase de Testing\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Resultado en Test:')\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
        "\n",
        "#mostrar pesos de la red\n",
        "#print(model.get_weights())\n",
        "\n",
        "#Guardar pesos y la arquitectura de la red en un archivo \n",
        "\n",
        "nombreArchivoModelo='arquitectura_prueba'\n",
        "nombreArchivoPesos='pesos_prueba'\n",
        "guardarRNN(model,nombreArchivoModelo,nombreArchivoPesos)\n",
        "\n",
        "#Cargar pesos y la arquitectura\n",
        "model2=cargarRNN(nombreArchivoModelo,nombreArchivoPesos) \n",
        "\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['acc']) #ADADELTA: An Adaptive Learning Rate Meth\n",
        "\n",
        "score = model2.evaluate(X_train, y_train, verbose=0)\n",
        "print('Resultado en Train:')\n",
        "print(\"%s: %.2f%%\" % (model2.metrics_names[1], score[1]*100))\n",
        "\n",
        "#Fase de Testing\n",
        "print('Resultado en Test:')\n",
        "score = model2.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model2.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 32)                224       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 2,596\n",
            "Trainable params: 2,596\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Resultado en Train:\n",
            "acc: 99.71%\n",
            "Resultado en Test:\n",
            "acc: 97.69%\n",
            "Guardando Red Neuronal en Archivo\n",
            "Red Neuronal Grabada en Archivo\n",
            "Red Neuronal Cargada desde Archivo\n",
            "Resultado en Train:\n",
            "acc: 99.71%\n",
            "Resultado en Test:\n",
            "acc: 97.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bY5pVuTk-lX",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">7. PREDICCION - USO DEL MODELO DE PRUEBA </span> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LOvPuahk-lZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c178856c-c342-469f-aa3e-d8fe3b9f2612"
      },
      "source": [
        "\n",
        "def predict(buying=0, maint=0, doors=0, persons=0, lug_boot=0, safety=0):\n",
        "    cnames = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']\n",
        "    data = [[buying, maint, doors, persons, lug_boot, safety]]\n",
        "    my_X = pd.DataFrame(data=data, columns=cnames)\n",
        "    my_X = preprocesador1.transform(my_X)\n",
        "    return model.predict_classes(my_X)\n",
        "\n",
        "print('Predicción:',predict(buying=2, maint=1, doors=5, persons=4, lug_boot=3, safety=3))\n",
        "print('Predicción:',predict(buying=1, maint=3, doors=5, persons=5, lug_boot=2, safety=1))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicción: [3]\n",
            "Predicción: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPgmNQy2k-lk",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">8. DISEÑO DE EXPERIMENTOS - OPTIMIZACION Y FINE TUNING </span> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jztdmZyEk-lm",
        "colab_type": "text"
      },
      "source": [
        "Para darnos mejor idea de cuanto es nuestro error, realizaremos un proceso llamado \"cross_validation\", este proceso entrena la cantidad de veces que definamos y devuelve la métrica indicada para todos los pasos, esto es mas que nada porque en cada epoch la pérdida suele variar, entonces esto nos dará la precisión calculando la media de todas estas precisiones.\n",
        "\n",
        "El proceso de Optimizacion consiste en reducir el error. Es decir, buscamos que la precision sea más alta. Mientras mayor es el accuracy será mejor el modelo de red neuronal para este problema.\n",
        "\n",
        "El proceso de \"Fine Tunning\" consiste en buscar posibles errores, y combinaciones de parámetros que puedan mejorar el modelo.\n",
        "Este proceso consume mucha memoria RAM, por lo tanto es recomendable usar alguna nube con mejores recursos a los locales, como por ejemplo: Google Colab. \n",
        "\n",
        "Enlaces: \n",
        "\n",
        "https://colab.research.google.com/notebooks/welcome.ipynb#recent=true\n",
        "\n",
        "https://colab.research.google.com/notebooks/\n",
        "\n",
        "Estrategia de Optimización: \n",
        "\n",
        "1.Compilación, \n",
        "\n",
        "2.Densidad de las capas de neuronas, y \n",
        "\n",
        "3.Agregar una cantidad de Dropout.\n",
        "\n",
        "Dropout: basicamente lo que hace es apagar neuronas al azar con el fin de que las neuronas no se vuelvan tan dependientes de\n",
        "los datos, es decir que se entrenen mejor para evitar el overfitting, para ello importaremos nuestra capa de dropout.\n",
        "La capa dropout recibe como parámetro un numero entre 0 y 1 que representa el porcentaje de neuronas que va a desactivar en \n",
        "esa capa, por el momento quedará en 0.1, luego optimizaremos ese valor.\n",
        "\n",
        "Con el modelo \"GridSearchCV\" podremos optimizar todos los parámetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATCLdd_Pk-lo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b010bba6-cfd5-474b-bd41-74f20ef42f9b"
      },
      "source": [
        "#importamos el algoritmo cross validator, y un wrapper que permitirá usar modelos de keras con scikit learn\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.layers import Dropout\n",
        "print('Librerías importadas')\n",
        "print(y_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Librerías importadas\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giCfnvbik-lw",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">9. DISEÑO DE RED NEURONAL DE BASE Y EVALUACION</span> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B72ShIbdk-l1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a49a2c5c-b864-451b-8c20-ddb4fe052d76"
      },
      "source": [
        "#Evaluación del modelo original: Esto puede tardar unos 30 segundos.\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(4, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "#El modelo se pasa como parámetro\n",
        "estimator = KerasClassifier(build_fn=build_model, epochs=100, batch_size=64) \n",
        "#cv es la cantidad de veces de entrenamiento del modelo\n",
        "#n_jobs es para ocupar mas de un procesador. El parámetro -1 indica que queremos utilizar todos los procesadores disponibles\n",
        "accuracies=cross_val_score(estimator, X_train, y_train, cv=10, n_jobs=-1)\n",
        "\n",
        "mean_acc=accuracies.mean()\n",
        "print('Precision media: ', mean_acc)\n",
        "\n",
        "##Como resultado al proceso de entrenamiento, tenemos un accuracy en promedio aprox. de: 0.4971. Intentaremos mejorarlo.\n",
        "#Simplemente con optimizer \"adam\", el resultado aprox. es: 0.798. Pruebenlo. Cambien optimizer='adadelta' a optimizer='adam'\n",
        "model=build_model() \n",
        "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=0)\n",
        "model.summary()\n",
        "print('Resultado en Train:')\n",
        "score = model.evaluate(X_train, y_train, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
        "\n",
        "#Fase de Testing\n",
        "print('Resultado en Test:')\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
        "\n",
        "nombreArchivoModelo='arquitectura_base'\n",
        "nombreArchivoPesos='pesos_base'\n",
        "guardarRNN(model,nombreArchivoModelo,nombreArchivoPesos)\n",
        "\n",
        "y = np.argmax(y, axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision media:  0.9797466456890106\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_16 (Dense)             (None, 32)                224       \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 2,596\n",
            "Trainable params: 2,596\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Resultado en Train:\n",
            "acc: 99.64%\n",
            "Resultado en Test:\n",
            "acc: 97.69%\n",
            "Guardando Red Neuronal en Archivo\n",
            "Red Neuronal Grabada en Archivo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_12S9n86k-mE",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">10. OPTIMIZACION Y FINE TUNING - COMPILACION </span> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82Y6pBVPk-mG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5a6e940c-e4c7-4ee1-8c2b-18d5cb62d13d"
      },
      "source": [
        "#1. Compilación: Prueba de mejores parámetros batch_size, epochs y optimizer\n",
        "#Esto recomiendo probarlo con Google Colab, puesto que se necesita 16GB en RAM y puede llegar a tardar unos 30min.\n",
        "\n",
        "\n",
        "def build_model(optimizer):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(4, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "\n",
        "#parámetros que queremos probar, y sus valores \n",
        "#probaremos con batch_size, epochs, y optimizador, con el fin de encontrar la mejor combinación entre estos tres parámetros.\n",
        "parameters = parameters = {'batch_size': [16,32],\n",
        "             'epochs':[100,200],\n",
        "             'optimizer': ['adadelta', 'rmsprop']}\n",
        "\n",
        "estimator = KerasClassifier(build_fn=build_model, verbose=0)\n",
        "#Ahora no le pasamos los parámetros al KerasClasifier, porque se los pasaremos a través de GridSearchCV\n",
        "#el argumento verbose=0 es para que no muestre salida, si lo dejamos en cero, no mostrará la barra de progreso del entrenamiento\n",
        "#GridSearchCV: recibe como parámetros nuestro modelo, nuestros parámetros, la medida sobre la que queremos comparar, y la \n",
        "#cantidad de veces que lo entrenará para sacar la media de accuracy.\n",
        "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10,n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(grid_search.best_params_)\n",
        "#Un ejemplo de resultados es: {'batch_size': 16, 'epochs': 100, 'optimizer': 'rmsprop'}\n",
        "#Esto indica que el optimizador \"adadelta\" no es adecuado. Y es que este optimizador NO sirve para este tipo de problemas."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 32, 'epochs': 200, 'optimizer': 'rmsprop'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o0R7oXNk-mO",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">11. OPTIMIZACION Y FINE TUNING - DENSIDAD DE LAS CAPAS DE NEURONAS </span> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waUvWXRik-mQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "de81c63e-571f-44e4-e23c-d4adc453cc59"
      },
      "source": [
        "#2. Densidad de las capas de neuronas\n",
        "#Notemos que se incluyen los mejores parámetros del paso de optimización anterior (batch_size, epochs y optimizer)\n",
        "#Esto recomiendo probarlo con Google Colab, puesto que se necesita 16GB en RAM y puede llegar a tardar unos 30min.\n",
        "def build_model(l1, l2):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(l1, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(l2, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(4, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "parameters = parameters = {'l1':[16,32,64,128,256],\n",
        "                           'l2':[16,23,64,128,256]}\n",
        "\n",
        "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=32, epochs=200)\n",
        "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10,n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "#Resultados: {'l1': 32, 'l2': 16}\n",
        "#Los resultados indican que hubo un error en la red original, las capas van desde la más densa, a la menos densa."
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'l1': 256, 'l2': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6DvYKwLk-ma",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">12. OPTIMIZACION Y FINE TUNING - PROCESO CON DROPOUTS </span> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0uL-ZN9k-mc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "60444635-27c1-46a4-d025-dd88a729bec6"
      },
      "source": [
        "#3. Proceso con dropouts: apagar un porcentaje de neuronas al azar con el fin de que las neuronas no se vuelvan tan \n",
        "#dependientes de los datos.\n",
        "##Esto recomiendo probarlo con Google Colab, puesto que se necesita 16GB en RAM y puede llegar a tardar unos 30min.\n",
        "def build_model(d1, d2):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "    model.add(Dropout(d1))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(d2))\n",
        "    model.add(Dense(4, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "parameters = parameters = {'d1':[0.1,0.2,0.3],\n",
        "                            'd2':[0.1,0.2,0.3]}\n",
        "\n",
        "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=32, epochs=200)\n",
        "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10,n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "#Resultados: {'d1':0.2, 'd2':0.3}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'d1': 0.1, 'd2': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgt_gIbjk-mm",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">13. EVALUACION DE MODELO OPTIMIZADO </span> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yae_IiKMk-mo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b13c9848-4273-436a-ae93-7d2067aedb8b"
      },
      "source": [
        "#Evaluación del Modelo Final\n",
        "\n",
        "finalModel = Sequential()\n",
        "#Finalmente, veamos como mejoró nuestro modelo, vamos a repetir el proceso de la validación cruzada.\n",
        "#Finalmente, veamos como mejoró nuestro modelo, vamos a repetir el proceso de la validación cruzada.\n",
        "#Esto puede probarse localmente. Ya con los mejores parámetros evaluamos la red neuronal. Puede tardar un minuto.\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(4, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=16, epochs=200)\n",
        "accuracies = cross_val_score(estimator, X_train, y_train, cv=10, n_jobs=-1)\n",
        "mean_acc = accuracies.mean()\n",
        "std_acc = accuracies.std()\n",
        "print('accuracies: ')\n",
        "print(accuracies)\n",
        "print('Precisión media: ', mean_acc)\n",
        "print('Desviación media: ',std_acc)\n",
        "#Y el resultado es:\n",
        "#Precision media aprox.: 0.8067\n",
        "#Pasamos de un 50% precisión a un 80% de precisión, por lo que se recomienda: \n",
        "#hacer siempre el proceso de fine tunning, porque ayudará a crear modelos correctos en la mayoría de los casos.\n",
        "\n",
        "y = df.y.values\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "encoded_y = encoder.transform(y)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "y = np_utils.to_categorical(encoded_y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
        "\n",
        "model=build_model() \n",
        "model.fit(X_train, y_train, epochs=200, batch_size=16, verbose=1)\n",
        "model.summary()\n",
        "\n",
        "score = model.evaluate(X_train, y_train, verbose=0)\n",
        "print('Resultado en Train:')\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
        "\n",
        "#Fase de Testing\n",
        "print('Resultado en Test:')\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
        "\n",
        "nombreArchivoModelo='arquitectura_optimizada'\n",
        "nombreArchivoPesos='pesos_optimizados'\n",
        "guardarRNN(model,nombreArchivoModelo,nombreArchivoPesos)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracies: \n",
            "[0.97841728 0.99280578 1.         1.         0.99275362 0.99275362\n",
            " 1.         0.97826087 1.         1.        ]\n",
            "Precisión media:  0.993499118089676\n",
            "Desviación media:  0.008201009376735503\n",
            "Epoch 1/200\n",
            "1382/1382 [==============================] - 0s 167us/step - loss: 0.5490 - acc: 0.7728\n",
            "Epoch 2/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.2733 - acc: 0.8777\n",
            "Epoch 3/200\n",
            "1382/1382 [==============================] - 0s 102us/step - loss: 0.1919 - acc: 0.9233\n",
            "Epoch 4/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.1552 - acc: 0.9443\n",
            "Epoch 5/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.1326 - acc: 0.9522\n",
            "Epoch 6/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.1156 - acc: 0.9573\n",
            "Epoch 7/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.1041 - acc: 0.9602\n",
            "Epoch 8/200\n",
            "1382/1382 [==============================] - 0s 105us/step - loss: 0.1008 - acc: 0.9602\n",
            "Epoch 9/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0865 - acc: 0.9667\n",
            "Epoch 10/200\n",
            "1382/1382 [==============================] - 0s 101us/step - loss: 0.0794 - acc: 0.9732\n",
            "Epoch 11/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0727 - acc: 0.9703\n",
            "Epoch 12/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 0.0718 - acc: 0.9725\n",
            "Epoch 13/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0631 - acc: 0.9747\n",
            "Epoch 14/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0577 - acc: 0.9776\n",
            "Epoch 15/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0518 - acc: 0.9805\n",
            "Epoch 16/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0558 - acc: 0.9761\n",
            "Epoch 17/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0491 - acc: 0.9826\n",
            "Epoch 18/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 0.0392 - acc: 0.9870\n",
            "Epoch 19/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0465 - acc: 0.9812\n",
            "Epoch 20/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0412 - acc: 0.9834\n",
            "Epoch 21/200\n",
            "1382/1382 [==============================] - 0s 107us/step - loss: 0.0424 - acc: 0.9855\n",
            "Epoch 22/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0315 - acc: 0.9855\n",
            "Epoch 23/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0377 - acc: 0.9855\n",
            "Epoch 24/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0316 - acc: 0.9891\n",
            "Epoch 25/200\n",
            "1382/1382 [==============================] - 0s 103us/step - loss: 0.0362 - acc: 0.9855\n",
            "Epoch 26/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 0.0310 - acc: 0.9863\n",
            "Epoch 27/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0377 - acc: 0.9834\n",
            "Epoch 28/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0288 - acc: 0.9899\n",
            "Epoch 29/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0265 - acc: 0.9899\n",
            "Epoch 30/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0238 - acc: 0.9906\n",
            "Epoch 31/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0296 - acc: 0.9863\n",
            "Epoch 32/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0258 - acc: 0.9884\n",
            "Epoch 33/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0240 - acc: 0.9899\n",
            "Epoch 34/200\n",
            "1382/1382 [==============================] - 0s 102us/step - loss: 0.0263 - acc: 0.9906\n",
            "Epoch 35/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0204 - acc: 0.9906\n",
            "Epoch 36/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0183 - acc: 0.9920\n",
            "Epoch 37/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0150 - acc: 0.9957\n",
            "Epoch 38/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0142 - acc: 0.9942\n",
            "Epoch 39/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0246 - acc: 0.9899\n",
            "Epoch 40/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0142 - acc: 0.9949\n",
            "Epoch 41/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0160 - acc: 0.9957\n",
            "Epoch 42/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0221 - acc: 0.9942\n",
            "Epoch 43/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0165 - acc: 0.9971\n",
            "Epoch 44/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0136 - acc: 0.9949\n",
            "Epoch 45/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0152 - acc: 0.9964\n",
            "Epoch 46/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0101 - acc: 0.9971\n",
            "Epoch 47/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0106 - acc: 0.9971\n",
            "Epoch 48/200\n",
            "1382/1382 [==============================] - 0s 108us/step - loss: 0.0112 - acc: 0.9957\n",
            "Epoch 49/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0156 - acc: 0.9949\n",
            "Epoch 50/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0083 - acc: 0.9964\n",
            "Epoch 51/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0116 - acc: 0.9957\n",
            "Epoch 52/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0066 - acc: 0.9986\n",
            "Epoch 53/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0170 - acc: 0.9964\n",
            "Epoch 54/200\n",
            "1382/1382 [==============================] - 0s 92us/step - loss: 0.0056 - acc: 0.9978\n",
            "Epoch 55/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0115 - acc: 0.9971\n",
            "Epoch 56/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0060 - acc: 0.9993\n",
            "Epoch 57/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0080 - acc: 0.9971\n",
            "Epoch 58/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0148 - acc: 0.9964\n",
            "Epoch 59/200\n",
            "1382/1382 [==============================] - 0s 118us/step - loss: 0.0068 - acc: 0.9986\n",
            "Epoch 60/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0093 - acc: 0.9949\n",
            "Epoch 61/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0052 - acc: 0.9986\n",
            "Epoch 62/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0059 - acc: 0.9971\n",
            "Epoch 63/200\n",
            "1382/1382 [==============================] - 0s 102us/step - loss: 0.0073 - acc: 0.9978\n",
            "Epoch 64/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0044 - acc: 0.9978\n",
            "Epoch 65/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0086 - acc: 0.9971\n",
            "Epoch 66/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0040 - acc: 0.9986\n",
            "Epoch 67/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0044 - acc: 0.9986\n",
            "Epoch 68/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0082 - acc: 0.9986\n",
            "Epoch 69/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0080 - acc: 0.9971\n",
            "Epoch 70/200\n",
            "1382/1382 [==============================] - 0s 110us/step - loss: 0.0048 - acc: 0.9978\n",
            "Epoch 71/200\n",
            "1382/1382 [==============================] - 0s 113us/step - loss: 0.0020 - acc: 0.9993\n",
            "Epoch 72/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0113 - acc: 0.9978\n",
            "Epoch 73/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0055 - acc: 0.9986\n",
            "Epoch 74/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0033 - acc: 0.9986\n",
            "Epoch 75/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 76/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0048 - acc: 0.9986\n",
            "Epoch 77/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 0.0126 - acc: 0.9971\n",
            "Epoch 78/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0052 - acc: 0.9978\n",
            "Epoch 79/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0024 - acc: 0.9993\n",
            "Epoch 80/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0054 - acc: 0.9993\n",
            "Epoch 81/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0045 - acc: 0.9986\n",
            "Epoch 82/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0028 - acc: 0.9993\n",
            "Epoch 83/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 84/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0065 - acc: 0.9971\n",
            "Epoch 85/200\n",
            "1382/1382 [==============================] - 0s 104us/step - loss: 0.0050 - acc: 0.9993\n",
            "Epoch 86/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 87/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 88/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0052 - acc: 0.9978\n",
            "Epoch 89/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0022 - acc: 0.9993\n",
            "Epoch 90/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0033 - acc: 0.9986\n",
            "Epoch 91/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0034 - acc: 0.9993\n",
            "Epoch 92/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0014 - acc: 0.9993\n",
            "Epoch 93/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0028 - acc: 0.9993\n",
            "Epoch 94/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 95/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0035 - acc: 0.9986\n",
            "Epoch 96/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0028 - acc: 0.9993\n",
            "Epoch 97/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0010 - acc: 1.0000\n",
            "Epoch 98/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0027 - acc: 0.9993\n",
            "Epoch 99/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0042 - acc: 0.9971\n",
            "Epoch 100/200\n",
            "1382/1382 [==============================] - 0s 102us/step - loss: 0.0036 - acc: 0.9993\n",
            "Epoch 101/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0056 - acc: 0.9986\n",
            "Epoch 102/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0015 - acc: 0.9993\n",
            "Epoch 103/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0028 - acc: 0.9993\n",
            "Epoch 104/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0019 - acc: 0.9993\n",
            "Epoch 105/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 2.8915e-04 - acc: 1.0000\n",
            "Epoch 106/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0112 - acc: 0.9971\n",
            "Epoch 107/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 4.9382e-04 - acc: 1.0000\n",
            "Epoch 108/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 109/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 6.6706e-04 - acc: 1.0000\n",
            "Epoch 110/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0040 - acc: 0.9993\n",
            "Epoch 111/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 0.0012 - acc: 0.9993\n",
            "Epoch 112/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0016 - acc: 0.9993\n",
            "Epoch 113/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0021 - acc: 0.9993\n",
            "Epoch 114/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0029 - acc: 0.9986\n",
            "Epoch 115/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 7.0781e-04 - acc: 1.0000\n",
            "Epoch 116/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 4.4146e-04 - acc: 1.0000\n",
            "Epoch 117/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0033 - acc: 0.9986\n",
            "Epoch 118/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0065 - acc: 0.9993\n",
            "Epoch 119/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 8.7958e-04 - acc: 0.9993\n",
            "Epoch 120/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 7.3962e-04 - acc: 1.0000\n",
            "Epoch 121/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 6.0011e-04 - acc: 1.0000\n",
            "Epoch 122/200\n",
            "1382/1382 [==============================] - 0s 101us/step - loss: 0.0159 - acc: 0.9986\n",
            "Epoch 123/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0012 - acc: 0.9993\n",
            "Epoch 124/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0021 - acc: 0.9993\n",
            "Epoch 125/200\n",
            "1382/1382 [==============================] - 0s 101us/step - loss: 0.0031 - acc: 0.9986\n",
            "Epoch 126/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0012 - acc: 0.9993\n",
            "Epoch 127/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0011 - acc: 0.9993\n",
            "Epoch 128/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 3.6734e-04 - acc: 1.0000\n",
            "Epoch 129/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0071 - acc: 0.9986\n",
            "Epoch 130/200\n",
            "1382/1382 [==============================] - 0s 102us/step - loss: 4.8103e-04 - acc: 1.0000\n",
            "Epoch 131/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0012 - acc: 0.9993\n",
            "Epoch 132/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 0.0052 - acc: 0.9986\n",
            "Epoch 133/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0061 - acc: 0.9978\n",
            "Epoch 134/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 4.7686e-04 - acc: 1.0000\n",
            "Epoch 135/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0034 - acc: 0.9978\n",
            "Epoch 136/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 3.3050e-04 - acc: 1.0000\n",
            "Epoch 137/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 6.4278e-04 - acc: 1.0000\n",
            "Epoch 138/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 2.3862e-04 - acc: 1.0000\n",
            "Epoch 139/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0018 - acc: 0.9993\n",
            "Epoch 140/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0012 - acc: 0.9993\n",
            "Epoch 141/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0012 - acc: 0.9993\n",
            "Epoch 142/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 1.0153e-04 - acc: 1.0000\n",
            "Epoch 143/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0099 - acc: 0.9986\n",
            "Epoch 144/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0030 - acc: 0.9993\n",
            "Epoch 145/200\n",
            "1382/1382 [==============================] - 0s 101us/step - loss: 2.3968e-04 - acc: 1.0000\n",
            "Epoch 146/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 5.6451e-04 - acc: 1.0000\n",
            "Epoch 147/200\n",
            "1382/1382 [==============================] - 0s 108us/step - loss: 4.1436e-04 - acc: 1.0000\n",
            "Epoch 148/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0012 - acc: 0.9993\n",
            "Epoch 149/200\n",
            "1382/1382 [==============================] - 0s 92us/step - loss: 0.0022 - acc: 0.9993\n",
            "Epoch 150/200\n",
            "1382/1382 [==============================] - 0s 92us/step - loss: 8.8896e-04 - acc: 1.0000\n",
            "Epoch 151/200\n",
            "1382/1382 [==============================] - 0s 92us/step - loss: 3.4446e-04 - acc: 1.0000\n",
            "Epoch 152/200\n",
            "1382/1382 [==============================] - 0s 101us/step - loss: 4.5855e-04 - acc: 1.0000\n",
            "Epoch 153/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 3.8070e-04 - acc: 1.0000\n",
            "Epoch 154/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0012 - acc: 0.9993\n",
            "Epoch 155/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 8.9283e-04 - acc: 1.0000\n",
            "Epoch 156/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 2.5198e-04 - acc: 1.0000\n",
            "Epoch 157/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0024 - acc: 0.9993\n",
            "Epoch 158/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 1.1612e-04 - acc: 1.0000\n",
            "Epoch 159/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 4.5547e-04 - acc: 1.0000\n",
            "Epoch 160/200\n",
            "1382/1382 [==============================] - 0s 101us/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 161/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 0.0048 - acc: 0.9993\n",
            "Epoch 162/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 4.1021e-04 - acc: 1.0000\n",
            "Epoch 163/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 0.0078 - acc: 0.9986\n",
            "Epoch 164/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 0.0061 - acc: 0.9986\n",
            "Epoch 165/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0031 - acc: 0.9993\n",
            "Epoch 166/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 4.7103e-04 - acc: 1.0000\n",
            "Epoch 167/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0015 - acc: 0.9993\n",
            "Epoch 168/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 1.3718e-04 - acc: 1.0000\n",
            "Epoch 169/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 4.9621e-05 - acc: 1.0000\n",
            "Epoch 170/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 4.9878e-04 - acc: 1.0000\n",
            "Epoch 171/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 0.0031 - acc: 0.9986\n",
            "Epoch 172/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 4.7555e-04 - acc: 1.0000\n",
            "Epoch 173/200\n",
            "1382/1382 [==============================] - 0s 91us/step - loss: 1.9039e-04 - acc: 1.0000\n",
            "Epoch 174/200\n",
            "1382/1382 [==============================] - 0s 91us/step - loss: 6.1635e-04 - acc: 1.0000\n",
            "Epoch 175/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 6.8335e-05 - acc: 1.0000\n",
            "Epoch 176/200\n",
            "1382/1382 [==============================] - 0s 92us/step - loss: 0.0026 - acc: 0.9986\n",
            "Epoch 177/200\n",
            "1382/1382 [==============================] - 0s 101us/step - loss: 0.0028 - acc: 0.9993\n",
            "Epoch 178/200\n",
            "1382/1382 [==============================] - 0s 91us/step - loss: 0.0093 - acc: 0.9978\n",
            "Epoch 179/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 0.0061 - acc: 0.9978\n",
            "Epoch 180/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 3.8321e-04 - acc: 1.0000\n",
            "Epoch 181/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0037 - acc: 0.9993\n",
            "Epoch 182/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 3.3532e-04 - acc: 1.0000\n",
            "Epoch 183/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 2.7897e-04 - acc: 1.0000\n",
            "Epoch 184/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 3.0945e-04 - acc: 1.0000\n",
            "Epoch 185/200\n",
            "1382/1382 [==============================] - 0s 97us/step - loss: 1.4046e-04 - acc: 1.0000\n",
            "Epoch 186/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 9.1996e-05 - acc: 1.0000\n",
            "Epoch 187/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 0.0027 - acc: 0.9993\n",
            "Epoch 188/200\n",
            "1382/1382 [==============================] - 0s 95us/step - loss: 8.7425e-04 - acc: 1.0000\n",
            "Epoch 189/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 4.6124e-05 - acc: 1.0000\n",
            "Epoch 190/200\n",
            "1382/1382 [==============================] - 0s 100us/step - loss: 9.4077e-04 - acc: 0.9993\n",
            "Epoch 191/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0022 - acc: 0.9986\n",
            "Epoch 192/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 0.0023 - acc: 0.9986\n",
            "Epoch 193/200\n",
            "1382/1382 [==============================] - 0s 99us/step - loss: 3.3635e-04 - acc: 1.0000\n",
            "Epoch 194/200\n",
            "1382/1382 [==============================] - 0s 96us/step - loss: 2.0386e-04 - acc: 1.0000\n",
            "Epoch 195/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 4.6069e-04 - acc: 1.0000\n",
            "Epoch 196/200\n",
            "1382/1382 [==============================] - 0s 94us/step - loss: 2.3686e-04 - acc: 1.0000\n",
            "Epoch 197/200\n",
            "1382/1382 [==============================] - 0s 93us/step - loss: 0.0014 - acc: 0.9993\n",
            "Epoch 198/200\n",
            "1382/1382 [==============================] - 0s 104us/step - loss: 0.0102 - acc: 0.9993\n",
            "Epoch 199/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 6.1116e-04 - acc: 1.0000\n",
            "Epoch 200/200\n",
            "1382/1382 [==============================] - 0s 98us/step - loss: 6.4925e-04 - acc: 1.0000\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_31 (Dense)             (None, 256)               1792      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 35,204\n",
            "Trainable params: 35,204\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Resultado en Train:\n",
            "acc: 100.00%\n",
            "Resultado en Test:\n",
            "acc: 99.42%\n",
            "Guardando Red Neuronal en Archivo\n",
            "Red Neuronal Grabada en Archivo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFTyVqoek-mw",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">14. COMPARACION ENTRE MODELO BASE Y MODELO OPTIMIZADO </span> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08m2hwg_k-mx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8c5feeed-2bcf-4e5e-8e38-eb9927862f1a"
      },
      "source": [
        "#Modelo Base\n",
        "print('MODELO BASE')\n",
        "nombreArchivoModelo='arquitectura_base'\n",
        "nombreArchivoPesos='pesos_base'\n",
        "Selectedmodel=cargarRNN(nombreArchivoModelo,nombreArchivoPesos) \n",
        "\n",
        "#Selectedmodel.summary()\n",
        "Selectedmodel.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['acc']) #ADADELTA: An Adaptive Learning Rate Method\n",
        "print('Resultado en Train:')\n",
        "score = Selectedmodel.evaluate(X_train, y_train, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (Selectedmodel.metrics_names[1], score[1]*100))\n",
        "\n",
        "#Fase de Testing\n",
        "print('Resultado en Test:')\n",
        "score = Selectedmodel.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (Selectedmodel.metrics_names[1], score[1]*100))\n",
        "\n",
        "#Modelo optimizado\n",
        "print('MODELO OPTIMIZADO')\n",
        "nombreArchivoModelo='arquitectura_optimizada'\n",
        "nombreArchivoPesos='pesos_optimizados'\n",
        "Selectedmodel=cargarRNN(nombreArchivoModelo,nombreArchivoPesos)    \n",
        "\n",
        "#Selectedmodel.summary()\n",
        "\n",
        "Selectedmodel.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['acc']) #ADADELTA: An Adaptive Learning Rate Method\n",
        "print('Resultado en Train:')\n",
        "score = Selectedmodel.evaluate(X_train, y_train, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (Selectedmodel.metrics_names[1], score[1]*100))\n",
        "\n",
        "#Fase de Testing\n",
        "print('Resultado en Test:')\n",
        "score = Selectedmodel.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (Selectedmodel.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODELO BASE\n",
            "Red Neuronal Cargada desde Archivo\n",
            "Resultado en Train:\n",
            "acc: 99.64%\n",
            "Resultado en Test:\n",
            "acc: 97.69%\n",
            "MODELO OPTIMIZADO\n",
            "Red Neuronal Cargada desde Archivo\n",
            "Resultado en Train:\n",
            "acc: 100.00%\n",
            "Resultado en Test:\n",
            "acc: 99.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzXz3mfok-m7",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:BLUE\">15. PREDICCION - USO DEL MODELO OPTIMIZADO </span> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euU4Rlyek-m9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c8d8abf6-f4c7-4abc-e101-27f3f301a4fb"
      },
      "source": [
        "#Predicciones con nuevos datos\n",
        "\n",
        "\n",
        "def predict(buying=0, maint=0, doors=0, persons=0, lug_boot=0, safety=0):\n",
        "    cnames = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']\n",
        "    data = [[buying, maint, doors, persons, lug_boot, safety]]\n",
        "    my_X = pd.DataFrame(data=data, columns=cnames)\n",
        "    my_X = preprocesador1.transform(my_X)\n",
        "    return Selectedmodel.predict_classes(my_X)\n",
        "\n",
        "print('Predicción:',predict(buying=2, maint=1, doors=5, persons=4, lug_boot=3, safety=3))\n",
        "print('Predicción:',predict(buying=1, maint=3, doors=5, persons=5, lug_boot=2, safety=1))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicción: [3]\n",
            "Predicción: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}